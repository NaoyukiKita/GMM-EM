{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM and EM algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduction\n",
    "\n",
    "<span style=\"color: red; \">GMM</span> (Gaussian Mixture Model) considers a probability density function as the sum of plural Gaussian distribution, and <span style=\"color: red; \">EM algorithm</span> (Expectation-Maximization algorithm) is one of iterative method of maximum likelihood estimation. Here we will estimate the parameters of GMM by EM algorithm based on observed sample dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Distribution\n",
    "\n",
    "The $D$-dimentional Gaussian distribution has a mean vector ${\\bf \\mu} \\in {\\mathbb R}^{D}$ and a variance-covariance matrix ${\\bf \\Lambda} \\in {\\mathbb R}^{(D \\times D)}$ as parameters. The function is expressed as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "N({\\bf x}|{\\bf \\mu}, {\\bf \\Sigma})\n",
    " = {(2 \\pi)}^{-\\frac{D}{2}}\n",
    "   {| {\\bf \\Sigma} |}^{-\\frac{1}{2}}\n",
    "   {\\rm exp}\\left\\{\n",
    "    -\\frac{1}{2} {({\\bf x} - {\\bf \\mu})}^{\\rm T} {\\bf \\Sigma}^{-1} ({\\bf x} - {\\bf \\mu})\n",
    "   \\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, we introduce ${\\bf \\Lambda}$ as the inverse matrix of ${\\bf \\Lambda}$ and rewrite the above function as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "N({\\bf x}|{\\bf \\mu}, {\\bf \\Lambda}^{-1})\n",
    " = {(2 \\pi)}^{-\\frac{D}{2}}\n",
    "   {| {\\bf \\Lambda}^{-1} |}^{-\\frac{1}{2}}\n",
    "   {\\rm exp}\\left\\{\n",
    "    -\\frac{1}{2} {({\\bf x} - {\\bf \\mu})}^{\\rm T} {\\bf \\Lambda} ({\\bf x} - {\\bf \\mu})\n",
    "   \\right\\}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mixture Gaussian Distribution\n",
    "\n",
    "<span style=\"color: red; \">The Mixture Gaussian Distribution</span> can be defined as the linear combination of the Gaussian Distribution like:\n",
    "\n",
    "$$\n",
    "p({\\bf x}) = \\sum_{k=1}^{K} \\pi_{k} N({\\bf x} | {\\bf \\mu}_{k}, {\\Lambda}_{k}^{-1})\n",
    "$$\n",
    "\n",
    "where $K$ is the number of the Gaussian (claster), and $\\pi_{k}$ corresponds to the probability of each cluster, so\n",
    "\n",
    "$$\n",
    "0 \\leq \\pi_{k} \\leq 1 \\quad \\quad \\sum_{k=1}^{K} \\pi_{k} = 1\n",
    "$$\n",
    "\n",
    "is satisfied.\n",
    "\n",
    "Now we have mixing ratios ${\\bf \\pi} = [\\pi_{1}, ..., \\pi_{K}]$, meanvectors ${\\bf \\mu} = [{\\bf \\mu}_{1}, ..., {\\bf \\mu}_{K}]$ and precision matrices ${\\bf \\Lambda} = [\\Lambda_{1}, ..., \\Lambda_{K}]$ as parameters to be estimated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The log likelihood of GMM\n",
    "\n",
    "We assume the observed sample ${\\bf x}_{n} \\in {\\mathbb R}^{D}, n=1,2,..., N$ follows the MGD, and we estimated the parameters ${\\bf \\pi}, {\\bf \\mu}, {\\bf \\Lambda}$ which maximize <span style=\"color: red; \">the log likelihood</span>:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\rm log} \\hspace{1mm} p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Lambda}^{-1}) &= \n",
    "    {\\rm log} \\prod_{n=1}^{N} p({\\bf x}_{n}) \\nonumber \\\\\n",
    " &= \\sum_{n=1}^{N} {\\rm log} \\hspace{1mm} p({\\bf x}_{n}) \\nonumber \\\\\n",
    " &= \\sum_{n=1}^{N} {\\rm log} \\sum_{k=1}^{K} \\pi_{k} N({\\bf x}_{n} | {\\bf \\mu}_{k}, {\\Lambda}_{k}^{-1}) \\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where ${\\bf X} = {[{\\bf x}_{1}, ..., {\\bf x}_{n}]}^{\\rm T} \\in {\\mathbb R}^{(N \\times D)}$ is sample set (observed variables). We want to solve the following equations:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        \\frac{\\partial}{\\partial {\\mu}_{m}} {\\rm log} \\hspace{1mm} p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\Lambda}_{k}^{-1}) = 0 \\\\\n",
    "        \\frac{\\partial}{\\partial {\\Lambda}_{m}} {\\rm log} \\hspace{1mm} p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\Lambda}_{k}^{-1}) = 0 \\\\\n",
    "        \\frac{\\partial}{\\partial {\\bf \\pi}_m} {\\rm log} \\hspace{1mm} p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\Lambda}_{k}^{-1}) = 0 \\\\\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$\n",
    "$$\n",
    "m^{\\forall} = 1, 2, ..., K\n",
    "$$\n",
    "\n",
    "Let us focus on how to solve them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The estimated parameters\n",
    "\n",
    "See **Reference** for detail.\n",
    "\n",
    "### ${\\mu}_{m}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial {\\mu}_{m}} {\\rm log} \\hspace{1mm} p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\Lambda}_{k}^{-1}) = \\sum_{n=1}^{N} {\\gamma}_{n,m} {\\Lambda}_{m} ({\\bf x}_{n} - {\\mu}_{m}) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{n=1}^{N} {\\gamma}_{n,m} {\\Lambda} {\\bf x}_{n} &= \\sum_{n=1}^{N} {\\gamma}_{n,m} {\\Lambda}_{m} {\\mu}_{m} \\nonumber \\\\\n",
    "\\sum_{n=1}^{N} {\\gamma}_{n,m} {\\bf x}_{n} &= \\sum_{n=1}^{N} {\\gamma}_{n,m} {\\mu}_{m} \\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore {\\mu}_{m} = \\frac{\\sum_{n=1}^{N} {\\gamma}_{n,m} {\\bf x}_{n}}{\\sum_{n=1}^{N} {\\gamma}_{n,m}}\n",
    "$$\n",
    "\n",
    "Here we introduce ${\\gamma}_{n,m}$ as <span style=\"color: red; \">responsibility</span>:\n",
    "\n",
    "$$\n",
    "{\\gamma}_{n,m} = \\frac{{\\bf \\pi}_{m} N({\\bf x}_{n} | {\\bf \\mu}_{m}, {\\Lambda}_{m}^{-1})}\n",
    "{\\sum_{k=1}^{K} {\\bf \\pi}_{k} N({\\bf x}_{n} | {\\bf \\mu}_{k}, {\\Lambda}_{k}^{-1})}\n",
    "$$\n",
    "\n",
    "It satisfies the following equation:\n",
    "\n",
    "$$\n",
    "\\sum_{m=1}^{K} {\\gamma}_{n,m} = \\frac{\\sum_{m=1}^{K} {\\bf \\pi}_{m} N({\\bf x}_{n} | {\\bf \\mu}_{m}, {\\Lambda}_{m}^{-1})}\n",
    "{\\sum_{k=1}^{K} {\\bf \\pi}_{k} N({\\bf x}_{n} | {\\bf \\mu}_{k}, {\\Lambda}_{k}^{-1})} = 1\n",
    "$$\n",
    "\n",
    "### ${\\Lambda}_{m}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial {\\Lambda}_{m}} {\\rm log} \\hspace{1mm} p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\Lambda}_{k}^{-1})\n",
    " = \\frac{1}{2} \\sum_{n=1}^{N} {\\gamma}_{n,m} \\{ {\\Lambda}_{m}^{-1} - ({\\bf x}_{n} - {\\bf \\mu}_{m}){({\\bf x}_{n} - {\\bf \\mu}_{m})}^{\\rm T} \\}\n",
    " = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore {\\Lambda}_{m}^{-1} = \\frac{\\sum_{n=1}^{N} {\\gamma}_{n,m} ({\\bf x}_{n} - {\\bf \\mu}_{m}){({\\bf x}_{n} - {\\bf \\mu}_{m})}^{\\rm T}}\n",
    " {\\sum_{n=1}^{N} {\\gamma}_{n,m}}\n",
    "$$\n",
    "\n",
    "\n",
    "### ${\\pi}_{m}$\n",
    "\n",
    "In order to satisfy the constraint $\\sum_{k=1}^{K} \\pi_{k} = 1$, <span style=\"color: red; \">the Lagrange multiplier method</span> is chosen.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial {\\bf \\pi}_m} \\left\\{ {\\rm log} \\hspace{1mm} p({\\bf X} | {\\bf \\pi}, {\\bf \\mu}, {\\Lambda}_{k}^{-1}) + \\lambda \\left( \\sum_{k=1}^{K} {\\pi}_k - 1 \\right) \\right\\} = \\frac{\\sum_{n=1}^{N} {\\gamma}_{n,m}}{{\\pi}_m} + \\lambda = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\pi}_{m} = - \\frac{1}{\\lambda} \\sum_{n=1}^{N} {\\gamma}_{n,m}\n",
    "$$\n",
    "\n",
    "We want to determin $\\lambda$. Using the constraint,\n",
    "\n",
    "$$\n",
    "\\sum_{m=1}^{K} \\pi_{m} =  -\\sum_{m=1}^{K} \\frac{1}{\\lambda} \\sum_{n=1}^{N} {\\gamma}_{n,m} = \\frac{1}{\\lambda} \\sum_{n=1}^{N} \\sum_{m=1}^{K} {\\gamma}_{n,m} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "-\\frac{1}{\\lambda} \\sum_{n=1}^{N} 1 = 1 \\quad (\\sum_{m=1}^{K} {\\gamma}_{n,m} = 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore \\lambda = -N\n",
    "$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "{\\pi}_{m} = - \\frac{1}{\\lambda} \\sum_{n=1}^{N} {\\gamma}_{n,m} = \\frac{1}{N} \\sum_{n=1}^{N} {\\gamma}_{n,m}\n",
    "$$\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "We can write the following system of simultaneous equations:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        {\\mu}_{m} = \\frac{\\sum_{n=1}^{N} {\\gamma}_{n,m} {\\bf x}_{n}}{\\sum_{n=1}^{N} {\\gamma}_{n,m}} \\\\\n",
    "        {\\Lambda}_{m}^{-1} = \\frac{\\sum_{n=1}^{N} {\\gamma}_{n,m} ({\\bf x}_{n} - {\\bf \\mu}_{m}){({\\bf x}_{n} - {\\bf \\mu}_{m})}^{\\rm T}} {\\sum_{n=1}^{N} {\\gamma}_{n,m}} \\\\\n",
    "        {\\pi}_{m} = \\frac{1}{N} \\sum_{n=1}^{N} {\\gamma}_{n,m} \\\\\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM algorithm\n",
    "\n",
    "${\\gamma}_{n,m}$:\n",
    "\n",
    "$$\n",
    "{\\gamma}_{n,m} = \\frac{{\\bf \\pi}_{m} N({\\bf x}_{n} | {\\bf \\mu}_{m}, {\\Lambda}_{m}^{-1})}\n",
    "{\\sum_{k=1}^{K} {\\bf \\pi}_{k} N({\\bf x}_{n} | {\\bf \\mu}_{k}, {\\Lambda}_{k}^{-1})}\n",
    "$$\n",
    "\n",
    "has ${\\mu}_{m}, {\\Lambda}_{m}, {\\pi}_{m}$ in its definition, while the estimated value of ${\\mu}_{m}, {\\Lambda}_{m}, {\\pi}_{m}$ have ${\\gamma}_{n,m}$. This makes the system of simultaneous equations non-linear. EM algorithm is one of the method of solving this iteratively.\n",
    "\n",
    "This method consists of two steps, namely <span style=\"color: red; \">E-step</span> and <span style=\"color: red; \">M-step</span>.\n",
    "\n",
    "We start from setting ${\\mu}_{m}^{(0)}, {\\Lambda}_{m}^{(0)}, {\\pi}_{m}^{(0)}$ at random.\n",
    "\n",
    "### E-step\n",
    "Calculate ${\\gamma}_{n,m}^{(t)}$ using ${\\mu}_{m}^{(t)}, {\\Lambda}_{m}^{(t)}, {\\pi}_{m}^{(t)}$.\n",
    "\n",
    "### M-step\n",
    "Calculate ${\\mu}_{m}^{(t+1)}, {\\Lambda}_{m}^{(t+1)}, {\\pi}_{m}^{(t+1)}$ using ${\\gamma}_{n,m}^{(t)}$ and update them.\n",
    "\n",
    "### Convergence criterion\n",
    "Repeate the above steps until the log likelihood:\n",
    "\n",
    "$$\n",
    "{\\rm log} \\hspace{1mm} p({\\bf X} | {\\bf \\pi}^{(t+1)}, {\\bf \\mu}^{(t+1)}, {\\bf \\Lambda}^{-1, (t+1)}) = \\sum_{n=1}^{N} {\\rm log} \\sum_{k=1}^{K} \\pi_{k}^{(t+1)} N({\\bf x}_{n} | {\\bf \\mu}_{k}^{(t+1)}, {\\Lambda}_{k}^{-1, (t+1)})\n",
    "$$\n",
    "\n",
    "converges.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "http://www.allisone.co.jp/html/Notes/Mathematics/statistics/gmm/index.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
